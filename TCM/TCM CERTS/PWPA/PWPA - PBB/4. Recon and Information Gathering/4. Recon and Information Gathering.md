BBP : [https://app.aikido.dev/login](https://app.aikido.dev/login)

## Fingerprint Web Technologies

### Web Site Technology :

- Wappalyzers
- BuiltWith
- Securityheaders.com

> [!info] Analyse your HTTP response headers  
> Quickly and easily assess the security of your HTTP response headers  
> [https://securityheaders.com/](https://securityheaders.com/)  

---

- curl
    
    - ==**curl -I {url}**==
    
    ```Bash
    ┌──(root㉿kali)-[~]
    └─# curl -I https://app.aikido.dev/login
    HTTP/2 200 
    date: Fri, 16 May 2025 06:25:38 GMT
    content-type: text/html; charset=UTF-8
    server: Apache
    content-security-policy: default-src 'self' https://cdn.aikido.dev; script-src 'self' https://widget.intercom.io https://js.intercomcdn.com 'sha256-dAMs3/Yp2SSUrhzjwbwLmPPB0soj/thHemUrM4u00O8=' https://cdn.aikido.dev https://js-eu1.hs-scripts.com https://js-eu1.hs-banner.com https://js-eu1.hubspot.com https://js-eu1.hs-analytics.net 'sha256-1e5RR2OpHhuX2h0Bat19DsNTmqbo4M3T1pqfeTXCHaA='; img-src 'self' data: blob: https://site-admin-avatar-cdn.prod.public.atl-paas.net/avatars/ https://api.atlassian.com/ex/jira/ https://static.intercomassets.com https://avatar-management--avatars.us-west-2.prod.public.atl-paas.net/ https://aikido-production-staticfiles-public.s3.eu-west-1.amazonaws.com/flags/ https://public.linear.app/3e49ffe7-6d8a-4461-87e6-51db22880264/b56a87f8-51d0-4b41-804e-9df63b3cafe7 https://*.intercomcdn.com https://static.intercomassets.com https://js.intercomcdn.com https://avatars.githubusercontent.com https://cdn.aikido.dev https://secure.gravatar.com https://track-eu1.hubspot.com/; media-src https://media.securecodewarrior.com/ https://video-bundler.lambdatest.com/; style-src 'unsafe-inline' 'self' https://cdn.aikido.dev; font-src 'self' https://*.intercomcdn.com https://cdn.aikido.dev; frame-src 'self' https://vaultjs.apideck.com https://www.aikido.dev/signup-succesful; connect-src 'self' https://o1405212.ingest.sentry.io/ https://*.intercom.io wss://*.intercom.io https://uploads.intercomcdn.com https://sockjs-eu.pusher.com wss://ws-eu.pusher.com https://cdn.aikido.dev https://cta-eu1.hubspot.com https://product.aikido.dev; prefetch-src 'self' https://cdn.aikido.dev/; object-src 'none'; frame-ancestors 'none'; form-action 'none'; base-uri 'self';
    x-ua-compatible: IE=Edge
    x-frame-options: SAMEORIGIN
    x-content-type-options: nosniff
    referrer-policy: no-referrer
    strict-transport-security: max-age=63072000; includeSubDomains
    ```
    
    - ==**curl -I -L {url} — ( for redirect )**==
    
    ```Bash
    ┌──(root㉿kali)-[~]
    └─# curl -I -L  https://app.aikido.dev/login
    HTTP/2 200 
    date: Fri, 16 May 2025 06:26:37 GMT
    content-type: text/html; charset=UTF-8
    server: Apache
    content-security-policy: default-src 'self' https://cdn.aikido.dev; script-src 'self' https://widget.intercom.io https://js.intercomcdn.com 'sha256-dAMs3/Yp2SSUrhzjwbwLmPPB0soj/thHemUrM4u00O8=' https://cdn.aikido.dev https://js-eu1.hs-scripts.com https://js-eu1.hs-banner.com https://js-eu1.hubspot.com https://js-eu1.hs-analytics.net 'sha256-1e5RR2OpHhuX2h0Bat19DsNTmqbo4M3T1pqfeTXCHaA='; img-src 'self' data: blob: https://site-admin-avatar-cdn.prod.public.atl-paas.net/avatars/ https://api.atlassian.com/ex/jira/ https://static.intercomassets.com https://avatar-management--avatars.us-west-2.prod.public.atl-paas.net/ https://aikido-production-staticfiles-public.s3.eu-west-1.amazonaws.com/flags/ https://public.linear.app/3e49ffe7-6d8a-4461-87e6-51db22880264/b56a87f8-51d0-4b41-804e-9df63b3cafe7 https://*.intercomcdn.com https://static.intercomassets.com https://js.intercomcdn.com https://avatars.githubusercontent.com https://cdn.aikido.dev https://secure.gravatar.com https://track-eu1.hubspot.com/; media-src https://media.securecodewarrior.com/ https://video-bundler.lambdatest.com/; style-src 'unsafe-inline' 'self' https://cdn.aikido.dev; font-src 'self' https://*.intercomcdn.com https://cdn.aikido.dev; frame-src 'self' https://vaultjs.apideck.com https://www.aikido.dev/signup-succesful; connect-src 'self' https://o1405212.ingest.sentry.io/ https://*.intercom.io wss://*.intercom.io https://uploads.intercomcdn.com https://sockjs-eu.pusher.com wss://ws-eu.pusher.com https://cdn.aikido.dev https://cta-eu1.hubspot.com https://product.aikido.dev; prefetch-src 'self' https://cdn.aikido.dev/; object-src 'none'; frame-ancestors 'none'; form-action 'none'; base-uri 'self';
    x-ua-compatible: IE=Edge
    x-frame-options: SAMEORIGIN
    x-content-type-options: nosniff
    referrer-policy: no-referrer
    strict-transport-security: max-age=63072000; includeSubDomains
    ```
    
    - More curl commands :
    
    ![[curl_commands]]
### ==Directory Enumeration and Burte Forceing :==

````Markdown
# Directory Enumeration Tools in Kali Linux

## Table of Contents
- [Introduction](#introduction)
- [Command-Line Tools](#command-line-tools)
  - [Gobuster](#gobuster)
  - [Dirsearch](#dirsearch)
  - [FFuF (Fuzz Faster U Fool)](#ffuf-fuzz-faster-u-fool)
  - [Dirb](#dirb)
  - [Wfuzz](#wfuzz)
  - [Feroxbuster](#feroxbuster)
- [GUI Tools](#gui-tools)
  - [Dirbuster](#dirbuster)
  - [Arachni](#arachni)
  - [Nikto](#nikto)
  - [Wapiti](#wapiti)
- [Wordlists](#wordlists)
- [Best Practices](#best-practices)
- [Additional Resources](#additional-resources)

## Introduction

Directory enumeration is a fundamental step in web application penetration testing. It involves discovering hidden 
files, directories, and resources on a web server that might expose sensitive information or functionality. This 
guide covers the best tools available in Kali Linux for directory enumeration.

## Command-Line Tools

### Gobuster
Fast and efficient directory and DNS subdomain brute-forcing tool.

```bash
# Basic directory brute-forcing
gobuster dir -u http://target.com -w /usr/share/wordlists/dirb/common.txt

# With extensions
gobuster dir -u http://target.com -w /usr/share/wordlists/dirb/common.txt -x php,txt,html

# Recursive scanning
gobuster dir -u http://target.com -w /usr/share/wordlists/dirb/common.txt -r

# Non-recursive scan (faster)
gobuster dir -u http://target.com -w /usr/share/wordlists/dirb/common.txt -r=false
```

### Dirsearch
Advanced web path scanner with many features.

```bash
# Basic scan
python3 dirsearch.py -u http://target.com -e php,html,txt

# Recursive scan
python3 dirsearch.py -u http://target.com -e php,html,txt -r

# With proxy
python3 dirsearch.py -u http://target.com -e php,html,txt --proxy http://127.0.0.1:8080

# Save output to file
python3 dirsearch.py -u http://target.com -e php,html,txt -o report.txt
```

### FFuF (Fuzz Faster U Fool)
Fast web fuzzer with great flexibility.

```bash
# Basic directory fuzzing
ffuf -u http://target.com/FUZZ -w /usr/share/wordlists/dirb/common.txt

# With extensions
ffuf -u http://target.com/FUZZ -w /usr/share/wordlists/dirb/common.txt -e .php,.txt,.html

# Recursive fuzzing
ffuf -u http://target.com/FUZZ -w /usr/share/wordlists/dirb/common.txt -recursion

# Filter by response size
ffuf -u http://target.com/FUZZ -w wordlist.txt -fs 0
```

### Dirb
Classic web content scanner.

```bash
# Basic scan
dirb http://target.com /usr/share/wordlists/dirb/common.txt

# With extensions
dirb http://target.com /usr/share/wordlists/dirb/common.txt -X .php,.txt

# Don't force extensions
dirb http://target.com /usr/share/wordlists/dirb/common.txt -X .php,.txt -N

# Set delay between requests
dirb http://target.com /usr/share/wordlists/dirb/common.txt -z 100
```

### Wfuzz
Web application fuzzer with many features.

```bash
# Basic directory fuzzing
wfuzz -c -z file,/usr/share/wordlists/dirb/common.txt --hc 404 http://target.com/FUZZ

# With extensions
wfuzz -c -z file,/usr/share/wordlists/dirb/common.txt -z list,"php-txt-html" -d "file=FUZZ.EXT" http://target.com

# Using multiple wordlists
wfuzz -c -z file,wordlist1.txt -z file,wordlist2.txt http://target.com/FUZZ1/FUZZ2
```

### Feroxbuster
Fast, recursive content discovery tool.

```bash
# Basic scan
feroxbuster -u http://target.com -w /usr/share/wordlists/dirb/common.txt

# With extensions
feroxbuster -u http://target.com -w /usr/share/wordlists/dirb/common.txt -x php,txt,html

# Set recursion depth
feroxbuster -u http://target.com -w wordlist.txt -d 3
```
````

````Markdown
## GUI Tools

### Dirbuster
GUI-based directory brute-forcing tool.

```bash
# Launch GUI version
dirbuster
```

**Features:**
- User-friendly interface
- Support for multiple wordlists
- Recursive scanning
- Results filtering
- Save/load scan sessions

### Arachni
Feature-rich web application security scanner.

```bash
# Basic scan
arachni http://target.com

# With specific checks
arachni --checks=common_files,backup_files http://target.com

# Save report
arachni http://target.com --report-save-path=report.afr
```

### Nikto
Web server scanner that checks for dangerous files and outdated server software.

```bash
# Basic scan
nikto -h http://target.com

# Scan with SSL
nikto -h https://target.com -ssl

# Save output
nikto -h http://target.com -o report.html -Format htm
```

### Wapiti
Web application vulnerability scanner with directory brute-forcing capabilities.

```bash
# Basic scan
wapiti -u http://target.com

# Scan with authentication
wapiti -u http://target.com --auth-method=basic --auth-user=admin --auth-password=password

# Generate HTML report
wapiti -u http://target.com -f html -o /path/to/report
```
````

````Markdown
## Wordlists

### Built-in Wordlists in Kali Linux
Kali Linux comes with several useful wordlists:

```
/usr/share/wordlists/dirb/common.txt
/usr/share/wordlists/dirb/big.txt
/usr/share/wordlists/dirb/small.txt
/usr/share/wordlists/dirb/extensions_common.txt
/usr/share/wordlists/dirbuster/directory-list-2.3-medium.txt
/usr/share/wordlists/seclists/Discovery/Web-Content/raft-small-directories.txt
/usr/share/wordlists/seclists/Discovery/Web-Content/raft-medium-directories.txt
/usr/share/wordlists/seclists/Discovery/Web-Content/raft-large-directories.txt
/usr/share/wordlists/seclists/Discovery/Web-Content/raft-large-files.txt
/usr/share/wordlists/seclists/Discovery/Web-Content/raft-large-words.txt
```

### Recommended Wordlist Resources

1. **SecLists**  
   [GitHub](https://github.com/danielmiessler/SecLists)  
   The most comprehensive collection of security wordlists, including directory brute-forcing, file names, and 
   common passwords.

2. **AssetNote Wordlists**  
   [Website](https://wordlists.assetnote.io/)  
   High-quality, regularly updated wordlists including directory brute-forcing and API endpoints.

3. **FuzzDB**  
   [GitHub](https://github.com/fuzzdb-project/fuzzdb)  
   Extensive collection of attack patterns and wordlists for various web vulnerabilities.

4. **Common Web Files**  
   [GitHub](https://github.com/danielmiessler/RobotsDisallowed)  
   Collection of paths found in robots.txt files.

5. **Bug Bounty Wordlist**  
   [GitHub](https://github.com/assetnote/commonspeak2-wordlists)  
   Generated from real-world data, great for bug bounty hunting.

6. **LFI Wordlist**  
   [GitHub](https://github.com/cujanovic/LFI-files-list)  
   Specifically for Local File Inclusion attacks.

### Wordlist Strategy

1. **Initial Recon**
   - Start with smaller wordlists (e.g., `common.txt`)
   - Look for common directories and files
   - Use with extensions (.php, .txt, .html, .js, etc.)

2. **Deep Scan**
   - Move to medium-sized wordlists (e.g., `raft-medium-directories.txt`)
   - Focus on technology-specific paths
   - Use recursive scanning for discovered directories

3. **Comprehensive Scan**
   - Use larger wordlists for thorough testing
   - Target specific technologies (CMS, frameworks, etc.)
   - Be mindful of server load and rate limiting

### Creating Custom Wordlists

1. **CeWL** - Generate wordlists from target websites:
   ```bash
   cewl -d 2 -m 5 -w custom_words.txt https://target.com
   ```

2. **Customize Existing Wordlists**:
   ```bash
   # Add common prefixes/suffixes
   for word in $(cat wordlist.txt); do echo "api-$word"; echo "dev-$word"; done > custom_wordlist.txt
   
   # Add common extensions
   for ext in php txt bak old; do sed "s/$/.$ext/" wordlist.txt; done > with_extensions.txt
   ```

3. **Combine Multiple Wordlists**:
   ```bash
   cat wordlist1.txt wordlist2.txt | sort -u > combined_wordlist.txt
   ```

### Wordlist Management Tips

- Always remove duplicates: `sort -u wordlist.txt > clean_wordlist.txt`
- Remove comments and empty lines: `grep -v '^#' wordlist.txt | grep -v '^$' > clean.txt`
- Sort by length: `awk '{print length, $0}' wordlist.txt | sort -n | cut -d' ' -f2- > sorted.txt`
- Split large wordlists: `split -l 10000 large_wordlist.txt split_`


## Best Practices

1. **Start Small**
   - Begin with smaller wordlists before moving to larger ones
   - Use common extensions first (.php, .html, .txt, .js, etc.)

2. **Be Stealthy**
   - Use rate limiting to avoid detection
   - Randomize user agents
   - Respect robots.txt (but don't rely on it for security)

3. **Check for Common Files**
   - Configuration files (.env, config.php, .git/config)
   - Backup files (.bak, .old, .backup, .swp)
   - Version control files (.git/, .svn/, .hg/)
   - Common administrative interfaces (/admin, /login, /cpanel)

4. **Analyze Results**
   - Look for interesting response codes (200, 301, 302, 403, 500)
   - Check for interesting file sizes (very small or very large responses)
   - Look for common patterns in responses

5. **Documentation**
   - Save your commands and results
   - Take screenshots of interesting findings
   - Document the date and time of your scans

6. **Legal Considerations**
   - Always get proper authorization before scanning
   - Be aware of local laws and regulations
   - Respect rate limits and terms of service

## Additional Resources

- [OWASP Testing Guide - Information Gathering](https://owasp.org/www-project-web-security-testing-guide/latest/4-Web_Application_Security_Testing/01-Information_Gathering/)
- [SecLists GitHub Repository](https://github.com/danielmiessler/SecLists)
- [Gobuster Documentation](https://github.com/OJ/gobuster)
- [FFuF Documentation](https://github.com/ffuf/ffuf)
- [Dirsearch GitHub](https://github.com/maurosoria/dirsearch)

---
````

```Markdown
# Domain vs Subdomain

## Domain
- The main, unique web address of a website (e.g., `example.com`)
- Also known as the root domain or primary domain
- Purchased and registered through domain registrars
- **Example:** `google.com`, `facebook.com`

## Subdomain
- A prefix added to the main domain (e.g., `blog.example.com`)
- Used to organize and navigate to different sections of a website
- Can be created by the domain owner without additional registration
- **Common uses:**
  - `blog.example.com` - For blog section
  - `shop.example.com` - For e-commerce
  - `app.example.com` - For web applications
  - `mail.example.com` - For email services

## Key Differences

| Aspect        | Domain                    | Subdomain                     |
|---------------|---------------------------|-------------------------------|
| Hierarchy     | Primary web address      | Part of the main domain       |
| Creation      | Requires registration    | Created by domain owner       |
| Purpose       | Main website             | Organizes different sections  |
| Security      | Primary security scope   | Can have separate settings    |
| Example       | `company.com`            | `blog.company.com`            |
```

1. ==Using advance google search [Dorking] :==
    1. ==[site:_domain.com_ -www]==
        1. ==[site:aikido.dev -www]==
2. ==Using== ==[crt.sh](http://crt.sh)== ==website :==
    1. ==%.aikido.dev==
3. ==Using assetfinder tool in kali :==
    1. ==assetfinder aikido.dev==
4. ==Using amass tool in kali== ==— ( takes long time to process )== ==:==
    
    1. ==amass enm -d== ==[aikido.dev](http://aikido.dev)== ==or amass enum -d aikido.dev -timeout 30 -max-dns-queries 5000 -o aikodo_sub.txt==
    

````Markdown
# Subdomain Enumeration Tools Comparison

## 1. Amass (Your Current Tool)
**Best for**: Comprehensive, in-depth enumeration  
**How it works**: Performs DNS enumeration, web archive scraping, and certificate transparency log checks  
**Best used when**: You need the most thorough results and have time to wait  

```bash
# Basic usage
amass enum -d example.com -o subs.txt

# Faster with limits
amass enum -d example.com -timeout 30 -max-dns-queries 5000 -o subs.txt
````

```Markdown
# 2. Subfinder
Best for: Fast, API-based enumeration
Strengths: Good balance of speed and coverage

# Install
go install -v github.com/projectdiscovery/subfinder/v2/cmd/subfinder@latest

# Basic usage
subfinder -d example.com -o subs.txt

# With API keys for better results
subfinder -d example.com -o subs.txt -t 100 -all
```

```Markdown
# 3. Assetfinder
Best for: Quick initial recon
Strengths: Very fast, good for initial discovery

# Install
go install github.com/tomnomnom/assetfinder@latest

# Basic usage
assetfinder -subs-only example.com > subs.txt
```

```Markdown
4. Findomain
Best for: Fastest initial results
Strengths: Extremely fast, written in Rust

# Install (Kali Linux)
sudo apt install findomain

# Basic usage
findomain -t example.com -o
# Output will be in example.com.txt
```

```Markdown
# Run tools in parallel
findomain -t udemy.com -o &
subfinder -d udemy.com -o subfinder.txt &
assetfinder --subs-only udemy.com > assetfinder.txt &
sublist3r -d udemy.com -o sublister.txt &
wait

# Process results
cat udemy.com.txt subfinder.txt assetfinder.txt sublister.txt 2>/dev/null | sort -u > final_subs.txt
cat final_subs.txt | httpx -title -status-code -silent -o live_subs.txt
```

  

## ==After sorting all your subdomain, you have to check if its live or not.==

  

````Markdown
# Subdomain Live Checking Guide

## Methods to Check Live Subdomains

### 1. Using `httpx` (Recommended)

**Installation:**
```bash
go install -v github.com/projectdiscovery/httpx/cmd/httpx@latest
```

**Basic Usage:**
```bash
# Basic check
cat aikodo_subs.txt | httpx -silent -o live_subs.txt

# With details (status code, title, content-length)
cat aikodo_subs.txt | httpx -title -content-length -status-code -o detailed_live_subs.txt

# With technology detection
cat aikodo_subs.txt | httpx -title -status-code -tech-detect -o tech_live_subs.txt
```

**Useful Flags:**
- `-threads`: Control number of threads (default: 50)
- `-timeout`: Timeout in seconds (default: 5)
- `-follow-redirects`: Follow HTTP redirects
- `-ports`: Specify custom ports to check (e.g., `-ports 80,443,8080,8443`)

### 2. Using `httprobe`

**Installation:**
```bash
go install github.com/tomnomnom/httprobe@latest  or apt install httprobe
```

**Basic Usage:**
```bash
# Check live subdomains (HTTP/HTTPS)
cat aikodo_subs.txt | httprobe -c 50 -t 3000 > live_subs.txt
```

**Useful Flags:**
- `-c`: Concurrency (default: 20)
- `-t`: Timeout in milliseconds (default: 10000)

### 3. Using `nmap` (For port scanning)

**Basic HTTP/HTTPS check:**
```bash
nmap -iL aikodo_subs.txt -p 80,443,8080,8443 --open -oG nmap_live.txt
```

**More comprehensive scan:**
```bash
nmap -iL aikodo_subs.txt -p 80,443,8080,8443,3000,8000,8081,8888 --open -sV --script=http-title -oA aikido_scan
```

### 4. Using `fping` (Quick ping check)

**Installation:**
```bash
sudo apt install fping
```

**Usage:**
```bash
cat aikodo_subs.txt | fping -a -g 2>/dev/null > live_hosts.txt
```

### 5. Using Python Script (No additional tools)

```python
#!/usr/bin/env python3
import requests
from concurrent.futures import ThreadPoolExecutor

def check_subdomain(subdomain):
    for protocol in ['http', 'https']:
        url = f"{protocol}://{subdomain}"
        try:
            r = requests.get(url, timeout=3, allow_redirects=True)
            print(f"[+] {url} - {r.status_code} - {len(r.content)} bytes")
            return f"{url} - {r.status_code}"
        except:
            pass
    print(f"[-] {subdomain} - Dead")
    return None

with open("aikodo_subs.txt") as f:
    subs = f.read().splitlines()

live = []
with ThreadPoolExecutor(max_workers=20) as executor:
    results = list(executor.map(check_subdomain, subs))
    live = [r for r in results if r]

with open("live_subs_py.txt", "w") as f:
    f.write("\n".join(live))
```

## Recommended Workflow

1. **First, use `httpx` for a quick check:**
   ```bash
   cat aikodo_subs.txt | httpx -title -status-code -content-length -o live_subs_detailed.txt
   ```

2. **For more detailed analysis:**
   ```bash
   # Get screenshots
   cat live_subs.txt | gowitness file -f - --threads 5

   # Or use eyewitness
   eyewitness -f live_subs.txt --web
   ```

3. **Check for common vulnerabilities:**
   ```bash
   # Check for exposed .git directories
   cat live_subs.txt | while read url; do
       curl -s "$url/.git/HEAD" | grep -q "ref:" && echo "$url - .git exposed"
   done
   ```

## Interpreting Results

- **Status Codes**:
  - `200`: OK - Page exists and is accessible
  - `301/302`: Redirect - Check where it redirects to
  - `403`: Forbidden - Access denied but resource exists
  - `404`: Not Found - Resource doesn't exist
  - `500`: Server Error - Potential vulnerability

- **Content Length**:
  - Very small responses (< 100 bytes) might be error pages
  - Very large responses might be data-heavy applications

- **Page Titles**:
  - Default server pages might indicate misconfiguration
  - Error messages in titles can reveal technology stack

## Tips for Effective Scanning

1. **Rate Limiting**: Add delay between requests to avoid being blocked
   ```bash
   cat aikodo_subs.txt | httpx -delay 1s
   ```

2. **Respect the Target**: Don't hammer servers with excessive requests

3. **Output Organization**: Keep results organized in separate files
   ```bash
   mkdir -p results/{screenshots,vulnerabilities,technologies}
   ```

4. **Parallel Processing**: For large lists, split and process in parallel
   ```bash
   split -l 100 aikodo_subs.txt split_
   ls split_* | xargs -P 4 -I {} sh -c 'cat {} | httpx -silent > {}.live'
   ```
````

# My Workflow for Subdomain Enumeration :

```Markdown
# Run tools in parallel

findomain -t udemy.com -o &
subfinder -d udemy.com -o subfinder.txt &
assetfinder --subs-only udemy.com > assetfinder.txt &
sublist3r -d udemy.com -o sublister.txt &
wait

# Process results
cat udemy.com.txt subfinder.txt assetfinder.txt sublister.txt 2>/dev/null | sort -u > final_subs.txt
cat final_subs.txt | httpx -title -status-code -silent -o live_subs.txt
```

# Use gowitness do get the pics of website :

```Markdown
go install github.com/sensepost/gowitness@latest  or  apt install gowitness

# Basic screenshot capture
cat final_sub.txt | gowitness file -f - --threads 20

# Or specify output directory
mkdir -p screenshots
cat final_sub.txt | gowitness file -f - --threads 20 --output-path screenshots/

# With custom settings
cat final_sub.txt | gowitness file -f - \
    --threads 20 \
    --timeout 10 \
    --resolution 1920x1080 \
    --output-path screenshots/ \
    --output-html screenshots/report.html

# Open the HTML report
firefox screenshots/report.html

# command : 

# First create the screenshots directory
mkdir -p screenshots

# Then run gowitness
gowitness -f final_sub.txt -t 30 -o screenshots/ -html screenshots/report.html
```

# Do this, remove the https:// from live domain before taking screenshot : 

```Markdown
# Create a new file with just the domains
cat final_sub.txt | sed 's/https:\/\///g' > clean_subdomains.txt

# Create a new file with both HTTP and HTTPS versions
cat final_sub.txt | sed 's/https:\/\///g' > clean_subdomains.txt
```

### Alternative for gowitness :
  

```Markdown
# Install eyewitness
git clone https://github.com/FortyNorthSecurity/EyeWitness.git
cd EyeWitness/Python/setup
./setup.sh

# Run eyewitness
python3 EyeWitness.py -f final_sub.txt --web -d screenshots/
```

or

```Markdown
# Create screenshots directory
mkdir -p screenshots

# Run httpx with screenshot
cat final_sub.txt | httpx -title -screenshot -o screenshots/

cat final_sub.txt | httpx -title -screenshot -silent -o screenshots/
```

